

# 🧩 선착순 이벤트 처리 구조 개선기
상황 : 웹사이트 페이지를 방문할 때마다 (ip, url,  시간대) 등 방문기록이 DB테이블에 insert 될때,

1. 선착순 이벤트로 특정 시간에 사용자가 몰릴때 RDB테이블에 짧은 타임에
과도한 insert로 예측되는 문제를 설명하고
2. 해결 방법을 제시
   
### 1️⃣ 문제 상황 

기존에는 **톰캣 스레드(Tomcat thread)** 가

* `손님 응대(이벤트 페이지 로직)`
* `주방 작업(DB insert 로직)`
  을 **모두 직접 처리**하고 있었다.

이때 선착순 이벤트가 열리면 특정 테이블(예: `event_entry`)에
`INSERT` 작업이 폭주하여 **B-Tree 인덱스의 리프 노드 페이지에 락 경쟁**이 발생한다.

* 첫 번째 `INSERT` 작업만 락을 획득하고 수행
* 나머지 `INSERT`들은 **DB 커넥션을 점유한 채 대기 상태**로 머무름
* 그 사이 새로운 `SELECT` 요청(다른 API 호출)이 들어와도
  DB 커넥션 풀에 여유가 없어 **커넥션을 얻지 못하고 대기**
* 결국 **서버 전체가 멈춘 듯한 상태**에 빠짐

즉,

> 톰캣 스레드가 ‘요리(INSERT)’하느라 ‘손님 응대(SELECT)’로 돌아가지 못한 상황이었다. 🍳

---

### 2️⃣ 문제 인식

톰캣 스레드가 동시에

* 주방(DB 작업)
* 홀(사용자 응답)
  을 둘 다 맡으니, 한쪽이 막히면 전체가 정체되는 구조였다.
  이를 해결하기 위해 **역할을 분리**해야 했다.

---

## 3️⃣ 해결 방법 — 솔루션  A : 큐(Queue)와 워커스레드(Worker Thread) 도입

💥 선착순 이벤트 폭주 시 발생 순서

1. 핫스팟 발생: INSERT가 DB 인덱스(B-Tree)의 마지막 리프 노드 페이지("1번 화구")에 몰림
2. 락 점유: "사장님 1"이 "1번 화구"(Lock)를 잡고 요리를 시작
3. 커넥션 고갈: "사장님 2~10"은 락이 풀리길 기다리며 DB 커넥션(냄비) 을 쥔 채 대기
4. 커넥션 풀 소진: "냄비 선반(DB 커넥션 풀)"은 0/10 상태로 고갈
5. 연쇄 붕괴 시작: INSERT와 무관한 SELECT API("샐러드 손님") 도착
6. "사장님 11"은 커넥션이 없어 작업 시작조차 불가
7. 결과: INSERT의 병목이 SELECT까지 마비시키는 연쇄 붕괴 발생

문제의 본질: "사장님(톰캣 스레드)"이 "느린 주방 일(DB 작업)"을 기다리느라
"냄비(DB 커넥션)"를 반납하지 못하고, "빠른 손님 응대(다른 API)"로 돌아가지 못하는 구조적 문제
**핵심 아이디어:**

> 톰캣 스레드는 DB 작업을 직접 하지 않고, “주문서만 큐에 올려두고” 바로 손님에게 응답한다.

1. **톰캣 스레드**

   * 사용자의 `insert` 요청을 받으면,
     작업 내용을 **큐(Queue)** 에 넣고 **즉시 응답** 반환.
   * 따라서 사용자 응답 속도 저하 없이 빠르게 처리 가능.

2. **워커스레드(Worker Thread)**

   * 큐에 쌓인 작업들을 순서대로 꺼내 DB에 반영
   * 각 작업은

     * 커넥션풀에서 커넥션을 **획득 → insert → 반납**
     * 그 다음 작업으로 이동
   * 예를 들어 **워커스레드 1개만** 두면,
     커넥션을 한 번에 하나씩만 사용하므로
     DB 커넥션풀 고갈 문제 해결 ✅

---

### 3️⃣-1 결과

| 문제        | 개선 전                     | 개선 후                |
| --------- | ------------------------ | ------------------- |
| DB 커넥션 고갈 | 여러 톰캣 스레드가 동시에 insert 대기 | 워커 1개가 순차 insert 처리 |
| 사용자 응답 지연 | insert 완료까지 톰캣 스레드 대기    | 큐에 맡기고 즉시 응답        |
| 서버 전체 정체  | insert로 인한 커넥션 점유        | 커넥션 사용 최소화          |

---

### 3️⃣-2 남은 과제

큐로 인해 **DB 커넥션 문제**는 해결되었지만,
여전히 `INSERT` 대상 인덱스 리프 노드가 집중되는 **핫스팟(Hotspot) 병목**은 남아 있다.

또한 `SELECT` 요청은 작업특성상 큐를 타지 않기 때문에,
만약 조회 쿼리 자체가 특정 리소스에 집중되는 경우는 어떻게 할까


> 톰캣 스레드가 “모든 걸 직접 처리하는 구조”에서
> “큐와 워커스레드로 역할을 분리한 구조”로 바꾸면서
> 응답성 문제와 DB 커넥션 고갈을 효과적으로 해결했다.
---
이후에는 **DB 인덱스 병목 완화**와
**읽기(SELECT) 트래픽 분산 전략**이 다음 단계 과제로 이어진다.

### ✅ 솔루션 A에 대한 내 고민들과 딥다이브

select작업이 일반적으로 빠른작업으로 알려져있지만, **느린 select작업이라면? select 핫스팟이 일어나면?**
elect 작업와 insert 작업의 작업성격의 차이가 존재하기때문에
톰캣스레드는 두 종류의 작업을 다르게 대한다.
select 작업은 데이터를 저장하는게 아니라 요청하는 작업이기에 큐에 들어가지 않는다.
그럼 select 핫스팟이 일어나면 큐로는 못해결하겠네. 그럼어떻게하지라는 고민.


<링크>

**반드시 큐 자료구조여야만 할까? 다른 자료구조가 대체 혹은 더 뛰어난 성능을 가지진 않을까? vs..**  
<링크>
**db커넥션풀 갯수??**

---
## 솔루션 A의 한계
<사진첨부>

## 4️⃣ 해결 방법 — 솔루션  B: BATCH
큐 주문서 꽂이에 대하여 워커스레드가 처리하는 방식을 바꾼다. 기존에는 insert작업이 큐에 들어오면 워커스레드가 db커넥션 획득 (가스밸브 오픈)- db insert작업 한개 처리 - db커넥션 반납 이 과정의 반복이였다.
그래서 생산자속도(카운터직원인 톰캣스레드)를 소비자속도(주방요리사인 워커스레드)가 따라가지 못해 큐에 작업에 계속 쌓이는 문제가 발생했다.
spring 프레임워크의 jdbctemplate기술을 이용해 워커스레드가 한개씩 처리하는게 아니라 n개씩 처리하게 만든다.
한번 db커넥션을 획득하여 초대형찜기(batchUpdate)를 이용해 만두1000개(insert작업 1000개)를 한번에 처리한다.


**batch_size 튜닝을 몇으로 해야 최적일까 ?**
->라는 고민은 할 지금 필요가 없어졌다.

log를 살펴보면 999개를 넘는게 없다. 한번에 처리할때 999개를 처리했다는 로그가 있어야. batch_size를 늘려서 tps를 더 최적화하거나 해볼텐데..
솔루션a보다 에러는 줄었다.

로그 샘플 10개 추출

V2보다 TPS가 10배 이상상 빨라졌다.
그중 한개를 보자.
[V3 BATCH] 377개의 로그 처리완료. 총시간: 255ms
계산: (377 / 255) * 1000 = 1,478 TPS 
1,573 TPS 생산자(카운터직원)속도
1,478 TPS 소비자(주방요리사)속도



1초마다 약 73개씩 "주문서"가 큐에 "계속 쌓여서", 결국 V2와 똑같이 **"느린 GC Hell"**이 발생했고, "손님(nGrinder)"이 **0.9%**만큼 **"Timeout(포기)"**한 겁니다.


<img width="802" height="367" alt="image" src="https://github.com/user-attachments/assets/ebe1e857-3792-4e1c-9b6d-b1ebaeaf5113" />

라고 결론내릴수있을까?

**로그 WORST CASE 샘플 추출**
[가장 운이 나쁠 때 (Worst-Case)]

[V3 BATCH] 12개의 로그 처리완료. 총시간: 83ms

계산: (12 / 83) * 1000 = 144 TPS

(V2(save) 때와 속도가 똑같습니다! "1000인용 찜기"로 "만두 12개"를 찌는 낭비를 한 겁니다.)
V2에 비해서 V3에서 에러율이 감소한것은 유의미한 결과라고 볼수있다.

V3가 V2보다 "1개 이상씩(Batch)" 처리했기 때문에, "비싼 비용(커넥션)"을 아낄 수 있었고, 그 결과 "요리사"의 **"평균 속도"**가 V2(142 TPS)보다 V3(778 TPS)가 훨씬 빨라짐.
그러나 WORST_CASE에 따르면 초당 처리량TPS이 유의미하게 향상되었다고는 볼수없다.
그럼 어떻게 해야할까.

V1(즉사) ➔ V2(큐) ➔ V3(배치)로 "해결"한 줄 알았지만, 그건 "해결"이 아니라 **"완화"**였습니다.

이상치의 함정인것인가?... tps는 향상된것이 아닐까? worst_Case(tps 144. v2와 비슷)는 왜 일어났고, 어떻게 해야해야하띾?

worst case가 나온 이유는 무엇일까? v2에 비해 문제가 완전히 해결되었다고 보긴 어려운상황.
v1으로 돌아가 db락 문제(핫스팟)를 건드려야하는것인가?
아니면 gc hell 문제를 건드려야 하는것인가?

______________________________________
4️⃣-B. "Error 0.9%"의 진짜 범인 찾기: DB 락 vs GC Hell
1. 첫 번째 가설 (Hypothesis A): "DB 락(핫스팟)"
V3 테스트 결과, Error 0.9%가 발생.

"요리 개수"와 "총 시간"이 비례하지 않는 **"이상 로그"**를 발견함.

[증거 A-1] 54개 처리 ➔ 260ms
[증거 A-2] 377개 처리 ➔ 255ms
[1차 결론] "요리 개수"와 상관없이 "0.25초"의 **"고정 락(DB 락)"**이 걸린다고 "추정"함. (V1의 핫스팟이 범인?-> pk에 걸려있던 인덱스를 uuid로 변경하는등의 방법을 고려?)

2. 가설 A의 "폐기" (반박 증거 발견)
하지만(But), "전체 로그"를 다시 분석하자 "가설 A"를 **"완벽하게 반박"**하는 **"초고속 로그"**가 발견됨.

[반박 증거 B-1] 80개 처리 ➔ 16ms (0.016초)
[반박 증거 B-2] 5개 처리 ➔ 16ms

[2차 결론] "DB 락"은 "범인"이 아님. (락이 있다면 "16ms"가 절대 불가능함.)

3. "진짜 범인" (Hypothesis B): "GC Hell (Stop-the-World)"
우리는 "두 개의 얼굴"을 가진 서버를 발견함.

"평소 (얼굴 1)": ~16ms로 "초고속"으로 작동함. (TPS 5,000)
"가끔 (얼굴 2)": ~250ms로 "멈춤".
"카운터(1573)"보다 "요리사(5000)"가 "훨씬 빠른데도" Error 0.9%가 뜸.
[최종 결론] 이 "모든 증거"를 설명할 범인은 **"GC Hell"**뿐.

"카운터"와 "요리사" 스레드 "모두"가 **"GC(Stop-the-World)"**에 **"0.25초간 '얼음'"**이 되면서, nGrinder(손님)는 "Timeout(Error 0.9%)"을 뱉고, "요리사"는 "250ms 멈춤 로그"를 남긴 것임.

4. 다음 단계 (V4)
"UUID(DB 락)"는 "가짜 보스"였음을 "데이터"로 증명함.

이 "GC Hell" 가설을 증명하기 위해, **jstat**을 이용해 "Full GC"가 0.25초씩 발생하는지 "측정"하기로 함.




## 5️⃣ 해결 방법 — 솔루션  C: kafka
방문기록처럼 유실되도 데이터가 아니라
금융기록처럼 유실되는 절대 안되는 데이터라면? 

## 기타 
1. 테이블 스키마에 대한 고민

2. 기업간 상황속에서 의 trade-off
- 처리속도
- 응답속도가 최우선으로 중요한다면
- 비용이 최우선으로 중요시한다면

빈 생성주기 postcontruct
캐싱
redis
## 6️⃣ Trade-off 분석 및 아키텍처 결정 - A B C
